# -*- coding: utf-8 -*-
"""13 - Approximation Q Control - Cart Pole.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aqpY15vazediaob2nHCKA0sjZtSrkXjm
"""

pip install gym==0.21.0

pip install pyglet

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.kernel_approximation import RBFSampler
from sklearn.preprocessing import PolynomialFeatures
import time
import gym

gym.version.VERSION



env = gym.make("CartPole-v0")

env.

s=env.reset()
s

a=env.action_space.sample() #random action
a

s_next, r, done, info = env.step(a)

env.reset()
done=False
while not done:
    a=env.action_space.sample() #random action
    s_next, r, done, info = env.step(a)
    time.sleep(0.05)
    env.render()
env.close()

def merge_state_action(s, a):
    return np.concatenate((s, [a]))

s=env.reset()
a=env.action_space.sample()
sa=merge_state_action(s, a)
sa

poly_features = PolynomialFeatures(degree=2)
poly_features.fit([sa])
poly_features.transform([sa])

def gather_samples(n_episodes=10000):
    samples = []
    for i in range(n_episodes):
        s = env.reset()
        done=False
        while not done:
            a=env.action_space.sample() #random action
            sa = merge_state_action(s, a)
            samples.append(sa)
            s, r, done, info = env.step(a)
    return samples

samples=gather_samples(n_episodes=10000)
samples

from sklearn.kernel_approximation import Nystroem
nystrom_featurizer = Nystroem(gamma=1,random_state=1,n_components=100)
nystrom_featurizer.fit(samples)
nystrom_featurizer.components_

rbf_feature = RBFSampler(n_components=100)
rbf_feature.fit(samples)

def predictQ(s,a,w,kernel):
    sa = merge_state_action(s, a)
    if kernel=='poly':
        poly_features.fit([sa])
        x=poly_features.transform([sa])[0]
    elif kernel=='rbf':
        x = rbf_feature.transform([sa])[0]
    else:
        x=nystrom_featurizer.transform([sa])[0]
    return np.dot(x,w)

def gradientQ(s,a,kernel):
    sa = merge_state_action(s, a)
    if kernel=='poly':
        poly_features.fit([sa])
        x=poly_features.transform([sa])[0]
    elif kernel=='rbf':
        x = rbf_feature.transform([sa])[0]
    else:
        x=nystrom_featurizer.transform([sa])[0]
    return x

env.action_space.n

def predictQ_all_actions(s,w,kernel):
    values=[]
    for a in range(env.action_space.n):
        values.append(predictQ(s,a,w,kernel))
    return values

def epsilon_greedy(s, w, kernel, eps=0.1):
    values=[]
    p = np.random.random()
    if p < (1 - eps):
        values = predictQ_all_actions(s,w,kernel)
        return np.argmax(values)
    else:
        return env.action_space.sample()

GAMMA=0.9
ALPHA = 0.1
#Initialize weights as zero
#RBFSampler has 100 dimensions as default, hence we have 100 features and weights
w = np.zeros(poly_features.n_output_features_) 
n_episodes = 1500
for it in range(n_episodes):
    # begin a new episode
    s=env.reset()
    done=False
    while not done:
        a = epsilon_greedy(s, w, kernel='poly',eps=0.1)
        s2, r, done, info = env.step(a)
        if done:
            target = r
        else:
            values = predictQ_all_actions(s2,w,kernel='poly')
            target = r + GAMMA * np.max(values)               
        # update the weights
        g = gradientQ(s,a,kernel='poly')
        err=target - predictQ(s,a,w,'poly')
        w += ALPHA * err * g
        # update state
        s = s2

w

def watch_agent(w,kernel):
    done = False
    episode_reward = 0
    s = env.reset()
    while not done:
        a = epsilon_greedy(s, w, kernel,eps=0)
        s, r, done, info = env.step(a)
        time.sleep(0.05)
        env.render()
        episode_reward += r
    print("Episode reward:", episode_reward)

watch_agent(w,kernel='poly')
env.close()

GAMMA=0.9
ALPHA = 0.1
#Initialize weights as zero
w = np.zeros(100) 
n_episodes = 500
for it in range(n_episodes):
    print(it)
    # begin a new episode
    s=env.reset()
    done=False
    while not done:
        a = epsilon_greedy(s, w, kernel='nystrom',eps=0.1)
        s2, r, done, info = env.step(a)
        if done:
            target = r
        else:
            values = predictQ_all_actions(s2,w,kernel='nystrom')
            target = r + GAMMA * np.max(values)               
        # update the weights
        g = gradientQ(s,a,kernel='nystrom')
        err=target - predictQ(s,a,w,'nystrom')
        w += ALPHA * err * g
        #print(w[0])
        # update state
        s = s2

w

watch_agent(w,kernel='nystrom')
env.close()

GAMMA=0.9
ALPHA = 0.1
#Initialize weights as zero
w = np.zeros(100) 
n_episodes = 1000
for it in range(n_episodes):
    print(it)
    # begin a new episode
    s=env.reset()
    done=False
    while not done:
        a = epsilon_greedy(s, w, kernel='rbf',eps=0.1)
        s2, r, done, info = env.step(a)
        if done:
            target = r
        else:
            values = predictQ_all_actions(s2,w,kernel='rbf')
            target = r + GAMMA * np.max(values)               
        # update the weights
        g = gradientQ(s,a,kernel='rbf')
        err=target - predictQ(s,a,w,'rbf')
        w += ALPHA * err * g
        # update state
        s = s2

watch_agent(w,kernel='rbf')
env.close()

