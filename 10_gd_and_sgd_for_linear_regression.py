# -*- coding: utf-8 -*-
"""10 - GD and SGD for Linear Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_8tv35D8o11XcEOcd3b4mhGrbNwmGHkM
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
mtcars = pd.read_csv("mtcars.csv")
mtcars=mtcars[['hp','disp','mpg']]
mtcars

y = mtcars["mpg"]
X = mtcars[["hp","disp"]]

def standardize(X):
    return (X - X.mean())/X.std(), X.mean(), X.std()

X,muX,sdX = standardize(X)
y,muy,sdy = standardize(y)

from sklearn.linear_model import LinearRegression

lr=LinearRegression()
lr.fit(X,y)

lr.intercept_

lr.coef_

def predict(X,w0,w):
    return w0 + np.dot(X,w)

def sse(y,ypred):
    return np.sum((ypred-y)**2)

"""Note that the linear regression model is
$$y=w_0+w_1x_1+w_2x_2$$
We want to minimize SSE
$$SSE=\sum_{i=1}^{n}[y-w_0-w_1x_{i1}-w_2x_{i2}]^2$$
Hence the gradients can be written as
$$\frac{\partial SSE}{\partial w_0}=2\sum_{i=1}^{n}[y-w_0-w_1x_{i1}-w_2x_{i2}]*-1$$
$$\frac{\partial SSE}{\partial w_1}=2\sum_{i=1}^{n}[y-w_0-w_1x_{i1}-w_2x_{i2}]*-x_{i1}$$
$$\frac{\partial SSE}{\partial w_2}=2\sum_{i=1}^{n}[y-w_0-w_1x_{i1}-w_2x_{i2}]*-x_{i2}$$
"""

def gradient(y,ypred,X,num_var):
    grad_w0 =-np.sum(y-ypred)
    grad_w = np.zeros(num_var)
    for j in range(num_var):
        grad_w[j]=-np.sum((y-ypred)*X.iloc[:,j])
    return grad_w0,grad_w

def update_param(w0,w,grad_w0,grad_w,alpha):
    w0_new = w0 - alpha * grad_w0
    w_new = np.zeros(len(w))
    for i in range(len(w)):
        w_new[i] = w[i] - alpha * grad_w[i]
    return w0_new,w_new

#Gradient Descent
w0 = 0
w = [0,0]
alpha = 0.005
num_iter = 1000
SSE_list = []
for i in range(num_iter):
    ypred = predict(X,w0,w)
    J = sse(y,ypred)
    SSE_list.append(J)
    grad_w0, grad_w = gradient(y,ypred,X,num_var=2)
    w0, w = update_param(w0,w,grad_w0,grad_w,alpha)

plt.plot(SSE_list)
plt.xscale('log')
print(w0,w)

"""Recall that in stochastic gradient descent we pass a single observation at a time, calculate the cost and update the parameters. Hence we are not obtaining the true direction that minimizes the sum of square errors, but may help us to obtain global optimum in case there are multiple local minimas.
$$Error=[y_i-w_0-w_1x_{i1}-w_2x_{i2}]^2$$
Hence the gradient at the point can be written as
$$\frac{\partial SSE}{\partial w_0}=2[y_i-w_0-w_1x_{i1}-w_2x_{i2}]*-1$$
$$\frac{\partial SSE}{\partial w_1}=2[y_i-w_0-w_1x_{i1}-w_2x_{i2}]*-x_{i1}$$
$$\frac{\partial SSE}{\partial w_2}=2[y_i-w_0-w_1x_{i1}-w_2x_{i2}]*-x_{i2}$$
"""

def gradientatSinglePoint(y,ypred,x,num_var):
    grad_w0 = -1*(y-ypred)
    grad_w = np.zeros(num_var)
    for j in range(num_var):
        grad_w[j]=-1*(y-ypred)*x[j]
    return grad_w0,grad_w

#Stochastic Gradient Descent
SSE_list = []
w0 = 0
w = [0,0]
alpha = 0.005
#pass over the whole data set 500 times
for k in range(1000):
    for i in range(len(X)):
        #Make prediction for one point
        ypred = predict(X.iloc[i,:],w0,w)
        #compute errors at that point
        J = sse(y[i],ypred)
        SSE_list.append(J)
        grad_w0, grad_w = gradientatSinglePoint(y[i],ypred,X.iloc[i,:],num_var=2)
        #update the parameters by using the gradient at single point
        w0, w = update_param(w0,w,grad_w0,grad_w,alpha)

plt.plot(SSE_list)
plt.xscale('log')
print(w0,w)

X.iloc[0,:]

