# -*- coding: utf-8 -*-
"""7 - TD Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z8aqzX4GuUToi0ZDruQ4vkSiiAxozc4F
"""

import matplotlib.pyplot as plt
import GridWorld
import numpy as np
import pandas as pd

robot=GridWorld.grid_world()

def print_values(V, rows,columns):
    for i in range(rows):
        print("---------------------------")
        for j in range(columns):
            v = V.get((i,j), 0)
            if v >= 0:
                print(" %.2f|" % v, end="")
            else:
                print("%.2f|" % v, end="") # -ve sign takes up an extra space
        print("")
def print_policy(policy,rows,columns):
    for i in range(rows):
        print("---------------------------")
        for j in range(columns):
            a = policy.get((i,j), ' ')
            print("  %s  |" % a, end="")
        print("")

### fixed policy ###
policy = {
    (2, 0): 'U',
    (1, 0): 'U',
    (0, 0): 'R',
    (0, 1): 'R',
    (0, 2): 'R',
    (1, 2): 'R',
    (2, 1): 'R',
    (2, 2): 'R',
    (2, 3): 'U',
  }

# initialize V(s)
V = {}
for s in robot.States:
    V[s] = 0

V

GAMMA=0.9
ALPHA = 0.1
# store max change in V(s) per episode
deltas = []
# repeat until convergence
n_episodes = 10000
for it in range(n_episodes):
    # begin a new episode
    s=robot.initial_state()
    delta = 0
    done=False
    while not done:
        a = policy[s]
        next_s, r, done=robot.step(a)
        # update V(s)
        v_old = V[s]
        V[s] = V[s] + ALPHA*(r + GAMMA*V[next_s] - V[s])
        delta = max(delta, np.abs(V[s] - v_old))
        # next state becomes current state
        s = next_s
        # store delta
        deltas.append(delta)

plt.plot(deltas)
plt.show()

print_policy(policy,3,4)

print_values(V,3,4)

